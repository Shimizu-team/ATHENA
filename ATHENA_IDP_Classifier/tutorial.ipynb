{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATHENA: Protein-Level IDP Classification Tutorial\n",
    "This Google Colab notebook provides a step-by-step guide for running the ATHENA protein-level classifier (ATHENA_IDP_classification.py).\n",
    "\n",
    "This tutorial will guide you through cloning the repository, setting up the environment, preparing data, and running the prediction pipeline to generate the ATHENA Score for your protein sequences.\n",
    "\n",
    "### Expected Runtime\n",
    "\n",
    "**Anywhere from 1-10 minutes**, depending on the number of sequences to be scored.\n",
    "\n",
    "### Step 1: Clone Repository and Set Up Environment\n",
    "First, we will clone the ATHENA GitHub repository and move into the new directory. Then, we'll install the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ATHENA'...\n",
      "remote: Repository not found.\n",
      "fatal: repository 'https://github.com/YOUR_USERNAME/ATHENA/' not found\n",
      "[Errno 2] No such file or directory: 'ATHENA'\n",
      "/Users/koheiota/Desktop/251028_ATHENA\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tqdm in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.23-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: psutil in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from peft) (7.1.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp311-cp311-macosx_10_9_x86_64.whl (290 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-macosx_10_9_x86_64.whl (11 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, safetensors, regex, MarkupSafe, hf-xet, filelock, jinja2, huggingface-hub, torch, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed MarkupSafe-3.0.3 accelerate-1.11.0 filelock-3.20.0 hf-xet-1.2.0 huggingface-hub-0.36.0 jinja2-3.1.6 mpmath-1.3.0 peft-0.17.1 regex-2025.10.23 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.2.2 transformers-4.57.1\n",
      "\n",
      "✅ Repository cloned and dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "# Colab Cell 1: Clone Repo & Install Dependencies\n",
    "\n",
    "# Clone the Repository\n",
    "!git clone https://github.com/Shimizu-team/ATHENA\n",
    "%cd ATHENA\n",
    "\n",
    "# Install Dependencies\n",
    "!pip install torch transformers peft tqdm einops\n",
    "\n",
    "print(\"\\n Repository cloned and dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "Now that we are in the repository's directory, we can import the necessary modules from your scripts, along with standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koheiota/anaconda3/envs/Tardigrada/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "from google.colab import files\n",
    "\n",
    "from config import Config\n",
    "from ATHENA_IDP_classification import predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Model Parameters and Input Data\n",
    "This step involves preparing all the necessary files for the model to run.\n",
    "\n",
    "**3. Unzip Model Parameters**\n",
    "\n",
    "Model parameters are in a zip file. The script expects the unzipped files to be in a directory named ATHENA_IDP_model_params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to unzip 'ATHENA_IDP_model_params/ATHENA_IDP_model_params.zip' into its parent directory 'ATHENA_IDP_model_params'...\n",
      "\n",
      "Extracting 'ATHENA_IDP_model_params/ATHENA_IDP_model_params.zip'...\n",
      "✅ Unzip command finished. Files extracted to 'ATHENA_IDP_model_params'.\n",
      "\n",
      "--- New contents of 'ATHENA_IDP_model_params' ---\n",
      "total 120736\n",
      "-rw-r--r--  1 koheiota  staff  29404325 Oct 28 10:31 ATHENA_IDP_model_params.zip\n",
      "-rwxr-xr-x@ 1 koheiota  staff       805 Oct 28 10:29 \u001b[31madapter_config.json\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff  31420456 Oct 28 10:29 \u001b[31madapter_model.safetensors\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff     17044 Oct 28 10:29 \u001b[31mclassifier_params.pth\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff     15024 Oct 28 10:29 \u001b[31mrng_state_0.pth\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff     15024 Oct 28 10:29 \u001b[31mrng_state_1.pth\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff     15024 Oct 28 10:29 \u001b[31mrng_state_2.pth\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff     15024 Oct 28 10:29 \u001b[31mrng_state_3.pth\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff      1064 Oct 28 10:29 \u001b[31mscheduler.pt\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff       171 Oct 28 10:29 \u001b[31mspecial_tokens_map.json\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff      3449 Oct 28 10:29 \u001b[31mtokenizer.json\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff      1399 Oct 28 10:29 \u001b[31mtokenizer_config.json\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff    871152 Oct 28 10:29 \u001b[31mtrainer_state.json\u001b[m\u001b[m\n",
      "-rwxr-xr-x@ 1 koheiota  staff      5432 Oct 28 10:29 \u001b[31mtraining_args.bin\u001b[m\u001b[m\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the directory and the zip file\n",
    "model_dir = \"ATHENA_IDP_model_params\"\n",
    "zip_filename = \"ATHENA_IDP_model_params.zip\"\n",
    "zip_filepath = os.path.join(model_dir, zip_filename)\n",
    "\n",
    "print(f\"Attempting to unzip '{zip_filepath}' into its parent directory '{model_dir}'...\")\n",
    "\n",
    "# Check if the zip file exists at the specified path\n",
    "if not os.path.isfile(zip_filepath):\n",
    "    print(f\"\\n⚠️ Error: File not found at '{zip_filepath}'.\")\n",
    "    print(\"Please ensure the GitHub repository was cloned correctly and this file exists.\")\n",
    "else:\n",
    "    print(f\"\\nExtracting '{zip_filepath}'...\")\n",
    "    \n",
    "    !unzip -q -o \"{zip_filepath}\" -d \"{model_dir}\"\n",
    "    \n",
    "    print(f\"Unzip command finished. Files extracted to '{model_dir}'.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Configuration ---\n",
      "{\n",
      "  \"output_type\": \"before_softmax\",\n",
      "  \"base_model\": \"Synthyra/ESMplusplus_small\",\n",
      "  \"classifier_params_path\": \"ATHENA_IDP_model_params\",\n",
      "  \"fasta_path\": \"input/example_sequences.fasta\",\n",
      "  \"output_dir\": \"output\",\n",
      "  \"adapter_paths\": {\n",
      "    \"IDP_LoRA\": \"ATHENA_IDP_model_params\"\n",
      "  },\n",
      "  \"adapter_weights\": null,\n",
      "  \"num_labels\": 2,\n",
      "  \"max_length\": 10000,\n",
      "  \"batch_size\": 32,\n",
      "  \"title\": \"IDP_Inference_Colab\"\n",
      "}\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Colab Cell 4: Define Configuration\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "manual_args = argparse.Namespace()\n",
    "\n",
    "# Basic Settings\n",
    "manual_args.output_type = \"before_softmax\" # \"IDP_probability\" or \"before_softmax\"\n",
    "manual_args.base_model = \"Synthyra/ESMplusplus_small\"\n",
    "manual_args.classifier_params_path = \"ATHENA_IDP_model_params\" # Relative path to unzipped folder\n",
    "manual_args.fasta_path = \"input/example_sequences.fasta\" # Relative path to created file\n",
    "manual_args.output_dir = \"output\"\n",
    "\n",
    "# Adapter Settings\n",
    "# Simulates: --adapter_paths \"IDP_LoRA=model_params\"\n",
    "manual_args.adapter_paths = {\"IDP_LoRA\": \"ATHENA_IDP_model_params\"} \n",
    "manual_args.adapter_weights = None # Not needed for a single adapter\n",
    "\n",
    "# Model Settings \n",
    "manual_args.num_labels = 2\n",
    "manual_args.max_length = 10000\n",
    "manual_args.batch_size = 32 # Adjust based on Colab GPU memory (e.g., T4)\n",
    "\n",
    "# Output Settings\n",
    "manual_args.title = \"IDP_Inference_Colab\" # Title for output files\n",
    "\n",
    "# Create the final config object\n",
    "conf = Config(manual_args)\n",
    "\n",
    "print(\"--- Running Configuration ---\")\n",
    "print(json.dumps(vars(conf), indent=2))\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run Prediction \n",
    "With all scripts imported, models loaded, and configuration set, we can now call the predict function.\n",
    "\n",
    "This will:\n",
    "\n",
    "1. Download the base model (Synthyra/ESMplusplus_small) from Hugging Face.\n",
    "\n",
    "2. Load the base model and apply your LoRA adapter from model_params.\n",
    "\n",
    "3. Load the fine-tuned classifier_params.pth.\n",
    "\n",
    "4. Process the sequences from example_sequences.fasta in batches.\n",
    "\n",
    "5. Save the results to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Prediction ---\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForSequenceClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_small and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single LoRA adapter 'IDP_LoRA'...\n",
      "Single LoRA adapter 'IDP_LoRA' loaded successfully.\n",
      "\n",
      "Model setup complete.\n",
      "Loaded classifier weights from ATHENA_IDP_model_params/classifier_params.pth\n",
      "Model is ready on cpu\n",
      "> 3 sequences loaded from input/example_sequences.fasta\n",
      "Starting prediction for 3 sequences with batch size 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting in batches:   0%|          | 0/1 [04:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mコードを実行できません。セッションは破棄されました。カーネルを再起動してください。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mコードを実行できません。セッションは破棄されました。カーネルを再起動してください。. \n",
      "\u001b[1;31m詳細については、Jupyter [ログ] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"--- Starting Prediction ---\")\n",
    "try:\n",
    "    # Call the main predict function from your script\n",
    "    predictions = predict(conf)\n",
    "    print(\"\\nPrediction Complete\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nExecution Error\")\n",
    "    print(e)\n",
    "    print(\"Prediction failed. Please double-check that your model files (e.g., 'classifier_params.pth')\")\n",
    "    print(\"are correctly located inside the 'model_params' directory after unzipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Review and Interpret Results\n",
    "The predict function saves the results as a .pt file, but also prints a summary. Let's load the saved file using torch and pandas for a cleaner view.\n",
    "\n",
    "Since we set --output_type \"before_softmax\", the output is the raw logit score (the \"ATHENA Score\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Determine the output file path based on our config\n",
    "if conf.output_type == \"before_softmax\":\n",
    "    output_file = os.path.join(conf.output_dir, f\"IDP_score_before_softmax_{conf.title}.pt\")\n",
    "    column_name = \"ATHENA Score (Logit)\"\n",
    "else:\n",
    "    output_file = os.path.join(conf.output_dir, f\"IDP_score_{conf.title}.pt\")\n",
    "    column_name = \"IDP Probability\"\n",
    "\n",
    "# Check if the file was created\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Loading results from '{output_file}'...\")\n",
    "    \n",
    "    # Load the saved dictionary (mapping to CPU for safety)\n",
    "    data = torch.load(output_file, map_location='cpu')\n",
    "    \n",
    "    # Convert to Pandas DataFrame for nice formatting\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Sequence ID', column_name])\n",
    "    \n",
    "    # Sort by score, descending\n",
    "    df_sorted = df.sort_values(by=column_name, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nPrediction Results (Top {len(df_sorted)})\")\n",
    "    \n",
    "    # Display as a clean markdown table\n",
    "    print(df_sorted.to_markdown(index=False))\n",
    "\n",
    "else:\n",
    "    print(f\"Output file '{output_file}' not found.\")\n",
    "    print(\"Please ensure Step 5 completed without errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tardigrada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
