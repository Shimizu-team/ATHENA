{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATHENA: Protein-Level IDP Classification Tutorial\n",
    "This Google Colab notebook provides a step-by-step guide for running the ATHENA protein-level classifier (ATHENA_IDP_classification.py).\n",
    "\n",
    "This tutorial will guide you through cloning the repository, setting up the environment, preparing data, and running the prediction pipeline to generate the ATHENA Score for your protein sequences.\n",
    "\n",
    "### Expected Runtime\n",
    "\n",
    "**Anywhere from 1-10 minutes**, depending on the number of sequences to be scored.\n",
    "\n",
    "### Step 1: Clone Repository and Set Up Environment\n",
    "First, we will clone the ATHENA GitHub repository and move into the new directory. Then, we'll install the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 1: Clone Repo & Install Dependencies\n",
    "\n",
    "# Clone the Repository\n",
    "!git clone https://github.com/Shimizu-team/ATHENA\n",
    "%cd ATHENA\n",
    "\n",
    "# Install Dependencies\n",
    "!pip install torch transformers peft tqdm einops\n",
    "\n",
    "print(\"\\n Repository cloned and dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "Now that we are in the repository's directory, we can import the necessary modules from your scripts, along with standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "from google.colab import files\n",
    "\n",
    "from config import Config\n",
    "from ATHENA_IDP_classification import predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Model Parameters and Input Data\n",
    "This step involves preparing all the necessary files for the model to run.\n",
    "\n",
    "**3. Unzip Model Parameters**\n",
    "\n",
    "Model parameters are in a zip file. The script expects the unzipped files to be in a directory named ATHENA_IDP_model_params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and the zip file\n",
    "model_dir = \"ATHENA_IDP_model_params\"\n",
    "zip_filename = \"ATHENA_IDP_model_params.zip\"\n",
    "zip_filepath = os.path.join(model_dir, zip_filename)\n",
    "\n",
    "print(f\"Attempting to unzip '{zip_filepath}' into its parent directory '{model_dir}'...\")\n",
    "\n",
    "# Check if the zip file exists at the specified path\n",
    "if not os.path.isfile(zip_filepath):\n",
    "    print(f\"\\n Error: File not found at '{zip_filepath}'.\")\n",
    "    print(\"Please ensure the GitHub repository was cloned correctly and this file exists.\")\n",
    "else:\n",
    "    print(f\"\\nExtracting '{zip_filepath}'...\")\n",
    "    \n",
    "    !unzip -q -o \"{zip_filepath}\" -d \"{model_dir}\"\n",
    "    \n",
    "    print(f\"Unzip command finished. Files extracted to '{model_dir}'.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 4: Define Configuration\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "manual_args = argparse.Namespace()\n",
    "\n",
    "# Basic Settings\n",
    "manual_args.output_type = \"before_softmax\" # \"IDP_probability\" or \"before_softmax\"\n",
    "manual_args.base_model = \"Synthyra/ESMplusplus_small\"\n",
    "manual_args.classifier_params_path = \"ATHENA_IDP_model_params\" # Relative path to unzipped folder\n",
    "manual_args.fasta_path = \"input/example_sequences.fasta\" # Relative path to created file\n",
    "manual_args.output_dir = \"output\"\n",
    "\n",
    "# Adapter Settings\n",
    "# Simulates: --adapter_paths \"IDP_LoRA=model_params\"\n",
    "manual_args.adapter_paths = {\"IDP_LoRA\": \"ATHENA_IDP_model_params\"} \n",
    "manual_args.adapter_weights = None # Not needed for a single adapter\n",
    "\n",
    "# Model Settings \n",
    "manual_args.num_labels = 2\n",
    "manual_args.max_length = 10000\n",
    "manual_args.batch_size = 32 # Adjust based on Colab GPU memory (e.g., T4)\n",
    "\n",
    "# Output Settings\n",
    "manual_args.title = \"IDP_Inference_Colab\" # Title for output files\n",
    "\n",
    "# Create the final config object\n",
    "conf = Config(manual_args)\n",
    "\n",
    "print(\"--- Running Configuration ---\")\n",
    "print(json.dumps(vars(conf), indent=2))\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run Prediction \n",
    "With all scripts imported, models loaded, and configuration set, we can now call the predict function.\n",
    "\n",
    "This will:\n",
    "\n",
    "1. Download the base model (Synthyra/ESMplusplus_small) from Hugging Face.\n",
    "\n",
    "2. Load the base model and apply your LoRA adapter from model_params.\n",
    "\n",
    "3. Load the fine-tuned classifier_params.pth.\n",
    "\n",
    "4. Process the sequences from example_sequences.fasta in batches.\n",
    "\n",
    "5. Save the results to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"--- Starting Prediction ---\")\n",
    "try:\n",
    "    # Call the main predict function from your script\n",
    "    predictions = predict(conf)\n",
    "    print(\"\\nPrediction Complete\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nExecution Error\")\n",
    "    print(e)\n",
    "    print(\"Prediction failed. Please double-check that your model files (e.g., 'classifier_params.pth')\")\n",
    "    print(\"are correctly located inside the 'model_params' directory after unzipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Review and Interpret Results\n",
    "The predict function saves the results as a .pt file, but also prints a summary. Let's load the saved file using torch and pandas for a cleaner view.\n",
    "\n",
    "Since we set --output_type \"before_softmax\", the output is the raw logit score (the \"ATHENA Score\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Determine the output file path based on our config\n",
    "if conf.output_type == \"before_softmax\":\n",
    "    output_file = os.path.join(conf.output_dir, f\"IDP_score_before_softmax_{conf.title}.pt\")\n",
    "    column_name = \"ATHENA Score (Logit)\"\n",
    "else:\n",
    "    output_file = os.path.join(conf.output_dir, f\"IDP_score_{conf.title}.pt\")\n",
    "    column_name = \"IDP Probability\"\n",
    "\n",
    "# Check if the file was created\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Loading results from '{output_file}'...\")\n",
    "    \n",
    "    # Load the saved dictionary (mapping to CPU for safety)\n",
    "    data = torch.load(output_file, map_location='cpu')\n",
    "    \n",
    "    # Convert to Pandas DataFrame for nice formatting\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Sequence ID', column_name])\n",
    "    \n",
    "    # Sort by score, descending\n",
    "    df_sorted = df.sort_values(by=column_name, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nPrediction Results (Top {len(df_sorted)})\")\n",
    "    \n",
    "    # Display as a clean markdown table\n",
    "    print(df_sorted.to_markdown(index=False))\n",
    "\n",
    "else:\n",
    "    print(f\"Output file '{output_file}' not found.\")\n",
    "    print(\"Please ensure Step 5 completed without errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tardigrada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
